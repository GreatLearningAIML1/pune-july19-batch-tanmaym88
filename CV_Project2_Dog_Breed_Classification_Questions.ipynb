{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "CV_Project2_Dog_Breed_Classification_Questions.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.5"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "2kIWaR5ZpKlJ"
      },
      "source": [
        "## Dog Breed Classification\n",
        "\n",
        "In this project we will use traditional CNN, CNN with data augmentation and finally transfer Learning by VGG16 model with weights pre-trained on Imagenet to solve the dog breed classification problem"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "F7MDmaAw2xGO"
      },
      "source": [
        "### Load Dataset Files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "BZWpQv1OwqYK",
        "colab": {}
      },
      "source": [
        "from google.colab import drive"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "fVhB9OopxFbX",
        "outputId": "576f167d-68ab-4ac7-bfae-885c989bc2d8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "1q2zzIaUprk_"
      },
      "source": [
        "Now, upload the given dataset file shared with you in your google drive and give its path for the below given `project_path` variable. For example, a path is given below according to the file path in our google drive. You need to change this to match the path of yours."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Tp6FvAToxUFs",
        "colab": {}
      },
      "source": [
        "project_path = \"/content/drive/My Drive/great-learning/computer-vision-project2/\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "rydR_j8lqUei"
      },
      "source": [
        "Run the below code to extract all the images in the train.zip files given in the dataset. We are going to use these images as train and validation sets and their labels in further steps."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "3350WZM4w4EL",
        "colab": {}
      },
      "source": [
        "from zipfile import ZipFile\n",
        "with ZipFile(project_path+'train.zip', 'r') as z:\n",
        "  z.extractall()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "3NHq1iBCfFjE"
      },
      "source": [
        "Repeat the same step for test.zip"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "_fxzynvB2YCb",
        "colab": {}
      },
      "source": [
        "from zipfile import ZipFile\n",
        "with ZipFile(project_path+'test.zip', 'r') as z:\n",
        "  z.extractall()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "jnUMhQrDfJmz"
      },
      "source": [
        "Repeat the same step for sample_submission.csv.zip"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "4PyTxE8q2jLf",
        "colab": {}
      },
      "source": [
        "from zipfile import ZipFile\n",
        "with ZipFile(project_path+'sample_submission.csv.zip', 'r') as z:\n",
        "  z.extractall()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "2G9RIxB-fOLT"
      },
      "source": [
        "Repeat the same step for labels.csv.zip"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "rXtnEoEixbgi",
        "colab": {}
      },
      "source": [
        "from zipfile import ZipFile\n",
        "with ZipFile(project_path+'labels.csv.zip', 'r') as z:\n",
        "  z.extractall()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "sJc1lVrW_jmL"
      },
      "source": [
        "After this process, we will have 4 files - Train folder, test folder and labels.csv and sample_submission.csv as part of your google drive"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "aYmJKmDqqpng"
      },
      "source": [
        "### Read labels.csv file using pandas"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "WmlJ2VMY96IZ",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "hPvb1RSc96If",
        "colab": {}
      },
      "source": [
        "labels = pd.read_csv('labels.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "QP8YAzQvqyK-"
      },
      "source": [
        "### Print the count of each category of Dogs given in the dataset\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "3L2naXlr96Im",
        "outputId": "61ba8898-a6ff-4215-dbe4-907f6c3746bd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "source": [
        "labels['breed'].value_counts()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "scottish_deerhound      126\n",
              "maltese_dog             117\n",
              "afghan_hound            116\n",
              "entlebucher             115\n",
              "bernese_mountain_dog    114\n",
              "                       ... \n",
              "golden_retriever         67\n",
              "komondor                 67\n",
              "brabancon_griffon        67\n",
              "eskimo_dog               66\n",
              "briard                   66\n",
              "Name: breed, Length: 120, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "WI94_Qcc0D4M"
      },
      "source": [
        "### Get one-hot encodings of labels"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Q48iAcY196I3",
        "colab": {}
      },
      "source": [
        "targets = pd.Series(labels['breed'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "9nlWmRNM96I8",
        "colab": {}
      },
      "source": [
        "one_hot_labels = np.asarray(pd.get_dummies(targets, sparse=True))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "VWaJ9naXfoiU"
      },
      "source": [
        "## Preparing training dataset\n",
        "1. Write a code which reads each and every id from labels.csv file and loads the corresponding image (in RGB - 128, 128, 3) from the train folder. <br>\n",
        "2. Create 2 variables <br> \n",
        "     a.  x_train - Should have all the images of the dogs from train folder <br>\n",
        "     b.  y_train - Corresponding label of the dog <br>\n",
        "<u>Note:</u> The id of the dog images and its corresponding labels are available in labels.csv file   \n",
        "<u>Hint:</u> Watch the video shared on \"Preparing the training dataset\" if you face issue on creating the training dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "aC2f9ecR0XGR",
        "colab": {}
      },
      "source": [
        "from tqdm import tqdm\n",
        "import cv2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "nkkZEpOe0ipk",
        "colab": {}
      },
      "source": [
        "x_train = []\n",
        "y_train = []\n",
        "img_rows = 128\n",
        "img_cols = 128"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "myk3Ej74IePr",
        "colab_type": "code",
        "outputId": "07aa2afc-7836-416c-efa9-7152f7ba16cc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "for f, img in tqdm(labels.values):\n",
        "  train_img = cv2.imread('./train/{}.jpg'.format(f),1)\n",
        "  train_img_resize = cv2.resize(train_img, (img_rows, img_cols))\n",
        "  x_train.append(train_img_resize)\n",
        "  y_train.append(img)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 10222/10222 [00:31<00:00, 324.85it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "6ioWDEgElBOs"
      },
      "source": [
        "Normalize the training data and convert into 4 dimensions so that it can be used as an input to conv layers in the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ARn76j3U1CDa",
        "colab": {}
      },
      "source": [
        "x_train_data = np.asarray(x_train)/255"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YsUMeHa7SPAa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_train_data = np.asarray(pd.get_dummies(y_train, sparse=True))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "bdCXuAE11gZL"
      },
      "source": [
        "### Split the training and validation data from `x_train_data` and `y_train_data` obtained from above step"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "kpWx-pgV96Jv",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cx38jNRbTcNi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_train, x_val, y_train, y_val= train_test_split(x_train_data, y_train_data, test_size=0.25, random_state=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HOlnFnHXYGdo",
        "colab_type": "code",
        "outputId": "2e32a4d6-7738-4d20-cedf-baaf87c0830d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "x_val.shape"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2556, 128, 128, 3)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "XkL-N1jDsU8m"
      },
      "source": [
        "### Loading the test data\n",
        "Read the id column from the samples_submission.csv and store it in test_img"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "DnpXdpd9b3E7",
        "colab": {}
      },
      "source": [
        "samples = pd.read_csv('sample_submission.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gnz1qtEcWFMl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_img = samples['id']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "DEJqZIMbm0Jo"
      },
      "source": [
        "Run the below code to load the test image files in x_test_feature"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "zf7n4WG-b3Hv",
        "outputId": "9eaafabf-38d2-4b6a-ef52-8b9a10d1b2da",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "x_test_feature = []\n",
        "i = 0 # initialisation\n",
        "for f in tqdm(test_img.values): # f for format ,jpg\n",
        "    img = cv2.imread('./test/{}.jpg'.format(f), 0)\n",
        "    img_resize = cv2.resize(img, (img_rows, img_cols)) \n",
        "    x_test_feature.append(img_resize)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 10357/10357 [00:14<00:00, 693.53it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "9My6qSyDnE-_"
      },
      "source": [
        "Normalize the test data and convert it into 4 dimensions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "93n-IntMnJGI",
        "colab": {}
      },
      "source": [
        "x_test_data = np.asarray(x_test_feature)/255"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "zKezNJVMsocP"
      },
      "source": [
        "### Build a basic conv neural network with 2 conv layers (kernel sizes - 5 and 3) add layers as mentioned below for classification.\n",
        "\n",
        "1. Add a Dense layer with 256 neurons with `relu` activation\n",
        "\n",
        "2. Add a Dense layer with 120 neurons as final layer (as there are 120 classes in the given dataset) with `softmax` activation for classifiaction. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "opqiyG3GXZO-",
        "colab_type": "code",
        "outputId": "d5181b6a-d720-44c7-a692-11e77529d317",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%tensorflow_version 2.x"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 2.x selected.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zldfbLiAXdf5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "D2jxTY2S96J4",
        "colab": {}
      },
      "source": [
        "#Clear any previous model from memory\n",
        "tf.keras.backend.clear_session()\n",
        "\n",
        "#Initialize model\n",
        "model = tf.keras.models.Sequential()\n",
        "\n",
        "model.add(tf.keras.layers.Reshape((128,128,3),input_shape=(128,128,3)))\n",
        "\n",
        "#Add Conv Layer\n",
        "model.add(tf.keras.layers.Conv2D(64, kernel_size=(5,5), activation='relu'))\n",
        "\n",
        "#Add Conv Layer\n",
        "model.add(tf.keras.layers.Conv2D(32, kernel_size=(3,3), activation='relu'))\n",
        "\n",
        "#Add Dense Layers after flattening the data\n",
        "model.add(tf.keras.layers.Flatten())\n",
        "\n",
        "model.add(tf.keras.layers.Dense(256, activation='relu'))\n",
        "\n",
        "#Add Output Layer\n",
        "model.add(tf.keras.layers.Dense(120, activation='softmax'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l6mhZquUXudq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zk0UEvdZXwq8",
        "colab_type": "code",
        "outputId": "090e0db5-4e56-4f65-9be2-d4e23c68869e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        }
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "reshape (Reshape)            (None, 128, 128, 3)       0         \n",
            "_________________________________________________________________\n",
            "conv2d (Conv2D)              (None, 124, 124, 64)      4864      \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 122, 122, 32)      18464     \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 476288)            0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 256)               121929984 \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 120)               30840     \n",
            "=================================================================\n",
            "Total params: 121,984,152\n",
            "Trainable params: 121,984,152\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ui8EXw6_oqpR"
      },
      "source": [
        "### Use batch_size = 128 and epochs = 10 and execute the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "IriIc37NozbK",
        "outputId": "8373cdf0-f601-4d8c-ad82-028ac0c3b748",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        }
      },
      "source": [
        "#Train the model\n",
        "model.fit(x_train,y_train,          \n",
        "          validation_data=(x_val,y_val),\n",
        "          epochs=10,\n",
        "          batch_size=128)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 7666 samples, validate on 2556 samples\n",
            "Epoch 1/10\n",
            "7666/7666 [==============================] - 35s 5ms/sample - loss: 5.7921 - accuracy: 0.0093 - val_loss: 4.7869 - val_accuracy: 0.0090\n",
            "Epoch 2/10\n",
            "7666/7666 [==============================] - 25s 3ms/sample - loss: 4.7900 - accuracy: 0.0130 - val_loss: 4.7861 - val_accuracy: 0.0094\n",
            "Epoch 3/10\n",
            "7666/7666 [==============================] - 25s 3ms/sample - loss: 4.7774 - accuracy: 0.0177 - val_loss: 4.7598 - val_accuracy: 0.0121\n",
            "Epoch 4/10\n",
            "7666/7666 [==============================] - 25s 3ms/sample - loss: 4.2501 - accuracy: 0.1224 - val_loss: 4.9846 - val_accuracy: 0.0168\n",
            "Epoch 5/10\n",
            "7666/7666 [==============================] - 25s 3ms/sample - loss: 2.4509 - accuracy: 0.4613 - val_loss: 6.5852 - val_accuracy: 0.0184\n",
            "Epoch 6/10\n",
            "7666/7666 [==============================] - 25s 3ms/sample - loss: 0.7441 - accuracy: 0.8448 - val_loss: 9.7391 - val_accuracy: 0.0215\n",
            "Epoch 7/10\n",
            "7666/7666 [==============================] - 25s 3ms/sample - loss: 0.1879 - accuracy: 0.9700 - val_loss: 12.1304 - val_accuracy: 0.0207\n",
            "Epoch 8/10\n",
            "7666/7666 [==============================] - 25s 3ms/sample - loss: 0.0706 - accuracy: 0.9914 - val_loss: 12.0021 - val_accuracy: 0.0200\n",
            "Epoch 9/10\n",
            "7666/7666 [==============================] - 25s 3ms/sample - loss: 0.0524 - accuracy: 0.9948 - val_loss: 12.2890 - val_accuracy: 0.0219\n",
            "Epoch 10/10\n",
            "7666/7666 [==============================] - 25s 3ms/sample - loss: 0.0431 - accuracy: 0.9935 - val_loss: 13.3110 - val_accuracy: 0.0211\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fd7c02330f0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Z8hWaKmjoz69"
      },
      "source": [
        "#The model accuracy is very poor !!!!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "agJKkc6xtKiq"
      },
      "source": [
        "### Use Data Augmentation in the above model to see if the accuracy improves\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "31Mn8qnZb3Ru",
        "colab": {}
      },
      "source": [
        "img_generator= tf.keras.preprocessing.image.ImageDataGenerator(rotation_range=20,\n",
        "                                                               width_shift_range=0.2,\n",
        "                                                               height_shift_range=0.2,\n",
        "                                                               horizontal_flip=True,\n",
        "                                                               validation_split=0.25)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "6sssbaTfxlkk"
      },
      "source": [
        "### Using the above objects, create the image generators with variable names `train_generator` and `val_generator`\n",
        "\n",
        "You need to use train_datagen.flow() and val_datagen.flow()"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "sehaRgT-96KQ",
        "colab": {}
      },
      "source": [
        "#Build training generator. \n",
        "train_generator = img_generator.flow(x_train_data,y_train_data, subset='training',  batch_size=128)\n",
        "\n",
        "#Build test generator\n",
        "val_generator = img_generator.flow(x_train_data,y_train_data, subset='validation', batch_size=128)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "TVFQJZw3x4-C"
      },
      "source": [
        "### Fit the model using fit_generator() using `train_generator` and `val_generator` from the above step with 10 epochs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "J1K2MqHbuPUa",
        "colab": {}
      },
      "source": [
        "#Saving the best model using model checkpoint callback\n",
        "model_checkpoint=tf.keras.callbacks.ModelCheckpoint('dogs_cnn.h5', \n",
        "                                                    save_best_only=True, \n",
        "                                                    monitor='val_accuracy', \n",
        "                                                    mode='max', \n",
        "                                                    verbose=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wOz6L54Fap_5",
        "colab_type": "code",
        "outputId": "3acfc3d7-e7fa-45fb-c074-997d4da182a0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 938
        }
      },
      "source": [
        "model.fit_generator(train_generator,\n",
        "                    epochs=10,\n",
        "                    steps_per_epoch= 7666//128,  #Number of training images//batch_size\n",
        "                    validation_data=val_generator,\n",
        "                    validation_steps = 2556//128, #Number of validation images//batch_size\n",
        "                    callbacks = [model_checkpoint]\n",
        "                    )"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-34-c1cf133b4a1c>:6: Model.fit_generator (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use Model.fit, which supports generators.\n",
            "WARNING:tensorflow:sample_weight modes were coerced from\n",
            "  ...\n",
            "    to  \n",
            "  ['...']\n",
            "WARNING:tensorflow:sample_weight modes were coerced from\n",
            "  ...\n",
            "    to  \n",
            "  ['...']\n",
            "Train for 59 steps, validate for 19 steps\n",
            "Epoch 1/10\n",
            "58/59 [============================>.] - ETA: 0s - loss: 5.0578 - accuracy: 0.0113\n",
            "Epoch 00001: val_accuracy improved from -inf to 0.01028, saving model to dogs_cnn.h5\n",
            "59/59 [==============================] - 44s 750ms/step - loss: 5.0532 - accuracy: 0.0114 - val_loss: 4.7834 - val_accuracy: 0.0103\n",
            "Epoch 2/10\n",
            "58/59 [============================>.] - ETA: 0s - loss: 4.7809 - accuracy: 0.0127\n",
            "Epoch 00002: val_accuracy did not improve from 0.01028\n",
            "59/59 [==============================] - 39s 655ms/step - loss: 4.7808 - accuracy: 0.0127 - val_loss: 4.7829 - val_accuracy: 0.0103\n",
            "Epoch 3/10\n",
            "58/59 [============================>.] - ETA: 0s - loss: 4.7786 - accuracy: 0.0131\n",
            "Epoch 00003: val_accuracy improved from 0.01028 to 0.01110, saving model to dogs_cnn.h5\n",
            "59/59 [==============================] - 44s 737ms/step - loss: 4.7787 - accuracy: 0.0129 - val_loss: 4.7832 - val_accuracy: 0.0111\n",
            "Epoch 4/10\n",
            "58/59 [============================>.] - ETA: 0s - loss: 4.7773 - accuracy: 0.0128\n",
            "Epoch 00004: val_accuracy did not improve from 0.01110\n",
            "59/59 [==============================] - 39s 656ms/step - loss: 4.7772 - accuracy: 0.0129 - val_loss: 4.7834 - val_accuracy: 0.0107\n",
            "Epoch 5/10\n",
            "58/59 [============================>.] - ETA: 0s - loss: 4.7760 - accuracy: 0.0131\n",
            "Epoch 00005: val_accuracy did not improve from 0.01110\n",
            "59/59 [==============================] - 39s 655ms/step - loss: 4.7761 - accuracy: 0.0131 - val_loss: 4.7854 - val_accuracy: 0.0099\n",
            "Epoch 6/10\n",
            "58/59 [============================>.] - ETA: 0s - loss: 4.7749 - accuracy: 0.0130\n",
            "Epoch 00006: val_accuracy did not improve from 0.01110\n",
            "59/59 [==============================] - 38s 645ms/step - loss: 4.7745 - accuracy: 0.0131 - val_loss: 4.7804 - val_accuracy: 0.0111\n",
            "Epoch 7/10\n",
            "58/59 [============================>.] - ETA: 0s - loss: 4.7671 - accuracy: 0.0146\n",
            "Epoch 00007: val_accuracy did not improve from 0.01110\n",
            "59/59 [==============================] - 39s 653ms/step - loss: 4.7673 - accuracy: 0.0146 - val_loss: 4.7828 - val_accuracy: 0.0103\n",
            "Epoch 8/10\n",
            "58/59 [============================>.] - ETA: 0s - loss: 4.7524 - accuracy: 0.0193\n",
            "Epoch 00008: val_accuracy improved from 0.01110 to 0.01562, saving model to dogs_cnn.h5\n",
            "59/59 [==============================] - 46s 778ms/step - loss: 4.7521 - accuracy: 0.0190 - val_loss: 4.7663 - val_accuracy: 0.0156\n",
            "Epoch 9/10\n",
            "58/59 [============================>.] - ETA: 0s - loss: 4.7467 - accuracy: 0.0185\n",
            "Epoch 00009: val_accuracy did not improve from 0.01562\n",
            "59/59 [==============================] - 38s 648ms/step - loss: 4.7474 - accuracy: 0.0186 - val_loss: 4.7722 - val_accuracy: 0.0123\n",
            "Epoch 10/10\n",
            "58/59 [============================>.] - ETA: 0s - loss: 4.7458 - accuracy: 0.0155\n",
            "Epoch 00010: val_accuracy did not improve from 0.01562\n",
            "59/59 [==============================] - 39s 655ms/step - loss: 4.7455 - accuracy: 0.0154 - val_loss: 4.7805 - val_accuracy: 0.0127\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fd7c00d76d8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Q2zmLztqo5DY"
      },
      "source": [
        "# Model accuracy is still poor!!!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "rSTATrhsAo7L"
      },
      "source": [
        "### Lets use Transfer Learning\n",
        "\n",
        "Download the vgg wieght file from here : https://github.com/MinerKasch/applied_deep_learning/blob/master/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "zy5JdbW6pIvD"
      },
      "source": [
        "Use the below code to load VGG16 weights trained on ImageNet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "yrqs0zg7ApNw",
        "colab": {}
      },
      "source": [
        "#from tf.keras.applications.vgg16 import VGG16, preprocess_input\n",
        "# Instantiate the model with the pre-trained weights (no top)\n",
        "base_model= tf.keras.applications.vgg16.VGG16(weights=project_path+'vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5',\n",
        "                 include_top=False, pooling='avg')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "EItOlRBGpV_A"
      },
      "source": [
        "Print the summary of the base_model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "lQsEBgnlpHjH",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 833
        },
        "outputId": "57a4c7a6-6bcc-459d-ed50-e297a6eb5ac4"
      },
      "source": [
        "base_model.summary()"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"vgg16\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         [(None, None, None, 3)]   0         \n",
            "_________________________________________________________________\n",
            "block1_conv1 (Conv2D)        (None, None, None, 64)    1792      \n",
            "_________________________________________________________________\n",
            "block1_conv2 (Conv2D)        (None, None, None, 64)    36928     \n",
            "_________________________________________________________________\n",
            "block1_pool (MaxPooling2D)   (None, None, None, 64)    0         \n",
            "_________________________________________________________________\n",
            "block2_conv1 (Conv2D)        (None, None, None, 128)   73856     \n",
            "_________________________________________________________________\n",
            "block2_conv2 (Conv2D)        (None, None, None, 128)   147584    \n",
            "_________________________________________________________________\n",
            "block2_pool (MaxPooling2D)   (None, None, None, 128)   0         \n",
            "_________________________________________________________________\n",
            "block3_conv1 (Conv2D)        (None, None, None, 256)   295168    \n",
            "_________________________________________________________________\n",
            "block3_conv2 (Conv2D)        (None, None, None, 256)   590080    \n",
            "_________________________________________________________________\n",
            "block3_conv3 (Conv2D)        (None, None, None, 256)   590080    \n",
            "_________________________________________________________________\n",
            "block3_pool (MaxPooling2D)   (None, None, None, 256)   0         \n",
            "_________________________________________________________________\n",
            "block4_conv1 (Conv2D)        (None, None, None, 512)   1180160   \n",
            "_________________________________________________________________\n",
            "block4_conv2 (Conv2D)        (None, None, None, 512)   2359808   \n",
            "_________________________________________________________________\n",
            "block4_conv3 (Conv2D)        (None, None, None, 512)   2359808   \n",
            "_________________________________________________________________\n",
            "block4_pool (MaxPooling2D)   (None, None, None, 512)   0         \n",
            "_________________________________________________________________\n",
            "block5_conv1 (Conv2D)        (None, None, None, 512)   2359808   \n",
            "_________________________________________________________________\n",
            "block5_conv2 (Conv2D)        (None, None, None, 512)   2359808   \n",
            "_________________________________________________________________\n",
            "block5_conv3 (Conv2D)        (None, None, None, 512)   2359808   \n",
            "_________________________________________________________________\n",
            "block5_pool (MaxPooling2D)   (None, None, None, 512)   0         \n",
            "_________________________________________________________________\n",
            "global_average_pooling2d (Gl (None, 512)               0         \n",
            "=================================================================\n",
            "Total params: 14,714,688\n",
            "Trainable params: 14,714,688\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "LeQem0pHITIj"
      },
      "source": [
        "### Make all the layers in the base_model (VGG16) to be non-trainable"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "0BpT4MLkqoaO",
        "colab": {}
      },
      "source": [
        "for layer in base_model.layers:\n",
        "    layer.trainable = False"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "fHpeOyW0qauW"
      },
      "source": [
        "### Add the following classification layers to the imported VGG Model <br>\n",
        "1. Flatten Layer\n",
        "2. Dense layer with 1024 neurons with activation as Relu\n",
        "3. Dense layer with 256 neurons with activation as Relu\n",
        "4. Dense layer with 120 neurons with activation as Softmax"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5rSoNttSSoMh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "1ecb7cc0-044b-4e79-9a75-fe650e836bbe"
      },
      "source": [
        "base_model.output"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor 'global_average_pooling2d/Identity:0' shape=(None, 512) dtype=float32>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "C7w9CSPvIRnX",
        "colab": {}
      },
      "source": [
        "#Create your own input format (here 3x200x200)\n",
        "#input = Input(shape=(3,200,200),name = 'image_input')\n",
        "\n",
        "#Use the generated model \n",
        "output_vgg16 = base_model.output\n",
        "\n",
        "#Add the fully-connected layers \n",
        "x = tf.keras.layers.Flatten(name='flatten')(output_vgg16)\n",
        "x = tf.keras.layers.Dense(1024, activation='relu', name='fc1')(x)\n",
        "x = tf.keras.layers.Dense(256, activation='relu', name='fc2')(x)\n",
        "x = tf.keras.layers.Dense(120, activation='softmax', name='predictions')(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fQnGxS5vRXnh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Using Keras Model class\n",
        "final_model = tf.keras.models.Model(inputs=base_model.input, #Pre-trained model input as input layer\n",
        "                                    outputs=x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8W5FGYXnTEir",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 969
        },
        "outputId": "72e30df7-2f2d-4522-9112-9eb431aab432"
      },
      "source": [
        "final_model.summary()"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         [(None, None, None, 3)]   0         \n",
            "_________________________________________________________________\n",
            "block1_conv1 (Conv2D)        (None, None, None, 64)    1792      \n",
            "_________________________________________________________________\n",
            "block1_conv2 (Conv2D)        (None, None, None, 64)    36928     \n",
            "_________________________________________________________________\n",
            "block1_pool (MaxPooling2D)   (None, None, None, 64)    0         \n",
            "_________________________________________________________________\n",
            "block2_conv1 (Conv2D)        (None, None, None, 128)   73856     \n",
            "_________________________________________________________________\n",
            "block2_conv2 (Conv2D)        (None, None, None, 128)   147584    \n",
            "_________________________________________________________________\n",
            "block2_pool (MaxPooling2D)   (None, None, None, 128)   0         \n",
            "_________________________________________________________________\n",
            "block3_conv1 (Conv2D)        (None, None, None, 256)   295168    \n",
            "_________________________________________________________________\n",
            "block3_conv2 (Conv2D)        (None, None, None, 256)   590080    \n",
            "_________________________________________________________________\n",
            "block3_conv3 (Conv2D)        (None, None, None, 256)   590080    \n",
            "_________________________________________________________________\n",
            "block3_pool (MaxPooling2D)   (None, None, None, 256)   0         \n",
            "_________________________________________________________________\n",
            "block4_conv1 (Conv2D)        (None, None, None, 512)   1180160   \n",
            "_________________________________________________________________\n",
            "block4_conv2 (Conv2D)        (None, None, None, 512)   2359808   \n",
            "_________________________________________________________________\n",
            "block4_conv3 (Conv2D)        (None, None, None, 512)   2359808   \n",
            "_________________________________________________________________\n",
            "block4_pool (MaxPooling2D)   (None, None, None, 512)   0         \n",
            "_________________________________________________________________\n",
            "block5_conv1 (Conv2D)        (None, None, None, 512)   2359808   \n",
            "_________________________________________________________________\n",
            "block5_conv2 (Conv2D)        (None, None, None, 512)   2359808   \n",
            "_________________________________________________________________\n",
            "block5_conv3 (Conv2D)        (None, None, None, 512)   2359808   \n",
            "_________________________________________________________________\n",
            "block5_pool (MaxPooling2D)   (None, None, None, 512)   0         \n",
            "_________________________________________________________________\n",
            "global_average_pooling2d (Gl (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "fc1 (Dense)                  (None, 1024)              525312    \n",
            "_________________________________________________________________\n",
            "fc2 (Dense)                  (None, 256)               262400    \n",
            "_________________________________________________________________\n",
            "predictions (Dense)          (None, 120)               30840     \n",
            "=================================================================\n",
            "Total params: 15,533,240\n",
            "Trainable params: 818,552\n",
            "Non-trainable params: 14,714,688\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "kj-BwqgfIkdv"
      },
      "source": [
        "### Fit and compile the model with batch_size = 128 and epochs = 10 and execute the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "YD5fAgVQIpKZ"
      },
      "source": [
        "Try to get training and validation accuracy to be more than 90%"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "SZk2SWvjIoRP",
        "colab": {}
      },
      "source": [
        "final_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oWynphKKTStx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Saving the best model using model checkpoint callback\n",
        "model_checkpoint=tf.keras.callbacks.ModelCheckpoint('dogs_vgg.h5', \n",
        "                                                    save_best_only=True, \n",
        "                                                    monitor='val_accuracy', \n",
        "                                                    mode='max', \n",
        "                                                    verbose=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iEub21vyTZxI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 867
        },
        "outputId": "6ed085b2-01c3-48a6-d9d5-6264f2ba757a"
      },
      "source": [
        "final_model.fit_generator(train_generator,\n",
        "                    epochs=10,\n",
        "                    steps_per_epoch= 7666//128,  #Number of training images//batch_size\n",
        "                    validation_data=val_generator,\n",
        "                    validation_steps = 2556//128, #Number of validation images//batch_size\n",
        "                    callbacks = [model_checkpoint]\n",
        "                    )"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:sample_weight modes were coerced from\n",
            "  ...\n",
            "    to  \n",
            "  ['...']\n",
            "WARNING:tensorflow:sample_weight modes were coerced from\n",
            "  ...\n",
            "    to  \n",
            "  ['...']\n",
            "Train for 59 steps, validate for 19 steps\n",
            "Epoch 1/10\n",
            "58/59 [============================>.] - ETA: 0s - loss: 3.0369 - accuracy: 0.2495\n",
            "Epoch 00001: val_accuracy did not improve from 0.19285\n",
            "59/59 [==============================] - 41s 692ms/step - loss: 3.0394 - accuracy: 0.2491 - val_loss: 3.4607 - val_accuracy: 0.1805\n",
            "Epoch 2/10\n",
            "58/59 [============================>.] - ETA: 0s - loss: 3.0129 - accuracy: 0.2604\n",
            "Epoch 00002: val_accuracy did not improve from 0.19285\n",
            "59/59 [==============================] - 40s 672ms/step - loss: 3.0089 - accuracy: 0.2613 - val_loss: 3.3799 - val_accuracy: 0.1879\n",
            "Epoch 3/10\n",
            "58/59 [============================>.] - ETA: 0s - loss: 2.9946 - accuracy: 0.2639\n",
            "Epoch 00003: val_accuracy improved from 0.19285 to 0.20025, saving model to dogs_vgg.h5\n",
            "59/59 [==============================] - 40s 686ms/step - loss: 2.9951 - accuracy: 0.2642 - val_loss: 3.3790 - val_accuracy: 0.2002\n",
            "Epoch 4/10\n",
            "58/59 [============================>.] - ETA: 0s - loss: 2.9051 - accuracy: 0.2801\n",
            "Epoch 00004: val_accuracy did not improve from 0.20025\n",
            "59/59 [==============================] - 41s 691ms/step - loss: 2.9040 - accuracy: 0.2799 - val_loss: 3.3558 - val_accuracy: 0.1928\n",
            "Epoch 5/10\n",
            "58/59 [============================>.] - ETA: 0s - loss: 2.8539 - accuracy: 0.2927\n",
            "Epoch 00005: val_accuracy improved from 0.20025 to 0.20477, saving model to dogs_vgg.h5\n",
            "59/59 [==============================] - 41s 693ms/step - loss: 2.8575 - accuracy: 0.2919 - val_loss: 3.3509 - val_accuracy: 0.2048\n",
            "Epoch 6/10\n",
            "58/59 [============================>.] - ETA: 0s - loss: 2.8448 - accuracy: 0.2875\n",
            "Epoch 00006: val_accuracy did not improve from 0.20477\n",
            "59/59 [==============================] - 41s 690ms/step - loss: 2.8406 - accuracy: 0.2882 - val_loss: 3.3841 - val_accuracy: 0.2002\n",
            "Epoch 7/10\n",
            "58/59 [============================>.] - ETA: 0s - loss: 2.7883 - accuracy: 0.3041\n",
            "Epoch 00007: val_accuracy did not improve from 0.20477\n",
            "59/59 [==============================] - 41s 689ms/step - loss: 2.7902 - accuracy: 0.3044 - val_loss: 3.3374 - val_accuracy: 0.1994\n",
            "Epoch 8/10\n",
            "58/59 [============================>.] - ETA: 0s - loss: 2.7905 - accuracy: 0.3055\n",
            "Epoch 00008: val_accuracy improved from 0.20477 to 0.22410, saving model to dogs_vgg.h5\n",
            "59/59 [==============================] - 41s 690ms/step - loss: 2.7914 - accuracy: 0.3048 - val_loss: 3.2878 - val_accuracy: 0.2241\n",
            "Epoch 9/10\n",
            "58/59 [============================>.] - ETA: 0s - loss: 2.7452 - accuracy: 0.3135\n",
            "Epoch 00009: val_accuracy did not improve from 0.22410\n",
            "59/59 [==============================] - 41s 687ms/step - loss: 2.7481 - accuracy: 0.3129 - val_loss: 3.3146 - val_accuracy: 0.2126\n",
            "Epoch 10/10\n",
            "58/59 [============================>.] - ETA: 0s - loss: 2.7248 - accuracy: 0.3168\n",
            "Epoch 00010: val_accuracy did not improve from 0.22410\n",
            "59/59 [==============================] - 40s 684ms/step - loss: 2.7248 - accuracy: 0.3169 - val_loss: 3.3432 - val_accuracy: 0.2007\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fcabd5f1dd8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    }
  ]
}